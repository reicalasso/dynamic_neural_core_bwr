# Configuration for a tiny model (~32M params) for quick tests on RTX 5060
model:
  vocab_size: 8000
  d_model: 512
  n_layers: 6
  n_heads: 8
  slots: 2048
  max_seq_len: 1024
  dropout: 0.1

training:
  batch_size: 16  # Higher batch size for smaller model
  accumulation_steps: 2  # Effective batch size of 32
  lr: 3e-4
  weight_decay: 0.01
  epochs: 5
  grad_clip: 1.0
  eval_every: 25
  use_wandb: false
  curriculum_learning: false
  
  # Fast training optimizations
  mixed_precision: true
  compile_model: true
  flash_attention: true
  gradient_checkpointing: false

performance:
  pin_memory: true
  num_workers: 2
  prefetch_factor: 2
  persistent_workers: true

project_name: "bwr-nsm-tiny"
