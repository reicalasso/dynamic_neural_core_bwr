# RTX 5060 Mobile optimized configuration (~200M params)
# Balanced for 8GB VRAM with maximum performance
model:
  vocab_size: 32000
  d_model: 1024
  n_layers: 16
  n_heads: 16
  slots: 8192
  max_seq_len: 4096
  dropout: 0.1

training:
  # Memory-optimized batch settings for RTX 5060
  batch_size: 4  # Small batch size due to large model
  accumulation_steps: 8  # Effective batch size of 32
  lr: 8e-5
  weight_decay: 0.01
  epochs: 20
  grad_clip: 0.5
  eval_every: 100
  use_wandb: true
  curriculum_learning: true
  
  # RTX 5060 specific optimizations
  mixed_precision: true  # Essential for 8GB VRAM
  compile_model: true    # PyTorch 2.0+ JIT compilation
  flash_attention: true  # Memory-efficient attention
  gradient_checkpointing: true  # Trade compute for memory
  cpu_offload: false     # Keep on GPU for speed
  
  # Advanced memory management
  empty_cache_steps: 50  # Clear cache periodically
  pin_memory: true
  non_blocking: true

# Learning rate scheduling
scheduler:
  type: "cosine_with_warmup"
  warmup_steps: 1000
  max_lr: 8e-5
  min_lr: 8e-6
  cycle_length: 5000

# Data loading optimizations
performance:
  pin_memory: true
  num_workers: 6  # Utilize CPU cores
  prefetch_factor: 3
  persistent_workers: true
  multiprocessing_context: "spawn"

# Model-specific optimizations
optimization:
  # Use bfloat16 for RTX 30+ series
  dtype: "bfloat16"  # Better than float16 for training stability
  
  # Attention optimizations
  attention_dropout: 0.0  # Disable for inference speed
  use_rotary_embedding: true
  use_alibi: false  # RoPE is better for long sequences
  
  # Memory bank optimizations
  state_compression: true
  compression_schedule: "adaptive"  # Compress based on memory pressure
  eviction_policy: "lru_salience"  # LRU + salience-based eviction

# Monitoring and checkpointing
monitoring:
  log_every: 10
  save_every: 500
  keep_checkpoints: 3
  monitor_memory: true
  monitor_throughput: true
  
  # W&B logging
  wandb_project: "bwr-dnc-rtx5060"
  wandb_tags: ["rtx5060", "mobile", "optimized"]

# Inference optimizations
inference:
  torch_compile: true
  channels_last: true  # Memory layout optimization
  cudnn_benchmark: true
  tf32: true  # Use TensorFloat-32 for speed

project_name: "bwr-dnc-rtx5060"
